{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ---------------------------\n",
        "# Output Directory Setup\n",
        "# ---------------------------\n",
        "output_dir = \"/content/drive/MyDrive/Edge-Pop_0.5_2/\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "log_file = os.path.join(output_dir, \"active_learning_log.csv\")\n",
        "weights_file = os.path.join(output_dir, \"global_lenet5.weights.h5\")\n",
        "final_subnet_weights = os.path.join(output_dir, \"final_subnet.weights.h5\")\n",
        "final_global_weights = os.path.join(output_dir, \"final_global_lenet5_full_training.weights.h5\")\n",
        "\n",
        "# ---------------------------\n",
        "# Edge-Popup Components\n",
        "# ---------------------------\n",
        "\n",
        "class GetSubnet(tf.keras.layers.Layer):\n",
        "    def __init__(self, k):\n",
        "        super(GetSubnet, self).__init__()\n",
        "        self.k = k\n",
        "\n",
        "    def call(self, scores):\n",
        "        scores_flat = tf.reshape(scores, [-1])\n",
        "        k_val = tf.cast(tf.size(scores_flat), tf.float32) * self.k\n",
        "        k_val = tf.cast(k_val, tf.int32)\n",
        "        k_val = tf.maximum(k_val, 1)\n",
        "\n",
        "        topk_values, topk_indices = tf.math.top_k(scores_flat, k=k_val, sorted=False)\n",
        "        mask_flat = tf.zeros_like(scores_flat)\n",
        "        mask_flat = tf.tensor_scatter_nd_update(\n",
        "            mask_flat, tf.expand_dims(topk_indices, 1), tf.ones_like(topk_values)\n",
        "        )\n",
        "        return tf.reshape(mask_flat, tf.shape(scores))\n",
        "\n",
        "class SubnetConv2D(tf.keras.layers.Layer):\n",
        "    def __init__(self, base_weights, base_bias, filters, kernel_size, strides=1, padding=\"same\", k=0.5, use_bias=True):\n",
        "        super(SubnetConv2D, self).__init__()\n",
        "        self.k = k\n",
        "        self.use_bias = use_bias\n",
        "        self.strides = strides\n",
        "        self.padding = padding.upper()\n",
        "\n",
        "        self.base_weights = base_weights\n",
        "        self.base_bias = base_bias\n",
        "        self.get_subnet = GetSubnet(k)\n",
        "\n",
        "        self.popup_scores = self.add_weight(\n",
        "            name=\"popup_scores\",\n",
        "            shape=base_weights.shape,\n",
        "            initializer=tf.keras.initializers.RandomNormal(stddev=0.1),\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        mask = self.get_subnet(tf.abs(self.popup_scores))\n",
        "        masked_weights = self.base_weights * mask\n",
        "        x = tf.nn.conv2d(inputs, masked_weights, strides=[1, self.strides, self.strides, 1], padding=self.padding)\n",
        "        if self.use_bias and self.base_bias is not None:\n",
        "            x = tf.nn.bias_add(x, self.base_bias)\n",
        "        return x\n",
        "\n",
        "def build_global_lenet5(input_shape=(32, 32, 3), num_classes=10):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(6, kernel_size=5, padding=\"same\", activation='tanh')(inputs)\n",
        "    x = layers.AveragePooling2D(pool_size=2)(x)\n",
        "    x = layers.Conv2D(16, kernel_size=5, activation='tanh')(x)\n",
        "    x = layers.AveragePooling2D(pool_size=2)(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(120, activation='tanh')(x)\n",
        "    x = layers.Dense(84, activation='tanh')(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    model = models.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "def build_edgepopup_subnet(global_model, k=0.5):\n",
        "    inputs = tf.keras.Input(shape=(32, 32, 3))\n",
        "\n",
        "    w1, b1 = global_model.layers[1].get_weights()\n",
        "    w2, b2 = global_model.layers[3].get_weights()\n",
        "\n",
        "    x = SubnetConv2D(w1, b1, 6, 5, k=k, padding='same')(inputs)\n",
        "    x = layers.Activation('tanh')(x)\n",
        "    x = layers.AveragePooling2D(pool_size=2)(x)\n",
        "\n",
        "    x = SubnetConv2D(w2, b2, 16, 5, k=k, padding='valid')(x)\n",
        "    x = layers.Activation('tanh')(x)\n",
        "    x = layers.AveragePooling2D(pool_size=2)(x)\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(120, activation='tanh')(x)\n",
        "    x = layers.Dense(84, activation='tanh')(x)\n",
        "    outputs = layers.Dense(10, activation='softmax')(x)\n",
        "    return tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# ---------------------------\n",
        "# Sampling Strategy\n",
        "# ---------------------------\n",
        "\n",
        "def least_confidence_sampling(model, unlabeled_pool, n_samples):\n",
        "    probs = model.predict(unlabeled_pool, verbose=0)\n",
        "    confidence = np.max(probs, axis=1)\n",
        "    return np.argsort(confidence)[:n_samples]\n",
        "\n",
        "# ---------------------------\n",
        "# Plotting\n",
        "# ---------------------------\n",
        "\n",
        "def plot_progress(accs, sizes):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(sizes, accs, 'o-')\n",
        "    plt.xlabel(\"Labeled Samples\")\n",
        "    plt.ylabel(\"Test Accuracy\")\n",
        "    plt.title(\"Active Learning Progress\")\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(output_dir, \"progress.png\"))\n",
        "    plt.close()\n",
        "\n",
        "def plot_comparison(global_acc, subnet_acc):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    models = ['Global LeNet-5', 'Final Subnet']\n",
        "    accuracies = [global_acc, subnet_acc]\n",
        "    plt.bar(models, accuracies)\n",
        "    plt.ylim([0, 1.0])\n",
        "    plt.ylabel(\"Test Accuracy\")\n",
        "    plt.title(\"Model Comparison after Full Training\")\n",
        "    plt.grid(True, axis='y')\n",
        "    for i, acc in enumerate(accuracies):\n",
        "        plt.text(i, acc + 0.01, f\"{acc:.4f}\", ha='center')\n",
        "    plt.savefig(os.path.join(output_dir, \"model_comparison.png\"))\n",
        "    plt.close()\n",
        "\n",
        "# ---------------------------\n",
        "# Active Learning Loop\n",
        "# ---------------------------\n",
        "\n",
        "def active_learning(global_model, x_train, y_train, x_test, y_test,\n",
        "                    k=0.5, init_size=1000, query_size=1000, iterations=10, epochs=10):\n",
        "\n",
        "    indices = np.arange(len(x_train))\n",
        "    labeled = np.random.choice(indices, size=init_size, replace=False)\n",
        "    unlabeled = np.setdiff1d(indices, labeled)\n",
        "\n",
        "    acc_hist = []\n",
        "    size_hist = []\n",
        "\n",
        "    popup_log = []\n",
        "    final_subnet_model = None  # To store the final subnet model\n",
        "\n",
        "    for i in range(iterations):\n",
        "        print(f\"\\n=== Iteration {i+1} ===\")\n",
        "        x_labeled, y_labeled = x_train[labeled], y_train[labeled]\n",
        "\n",
        "        model = build_edgepopup_subnet(global_model, k)\n",
        "        model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "        model.fit(x_labeled, y_labeled, validation_split=0.1,\n",
        "                  epochs=epochs, batch_size=128, verbose=1)\n",
        "\n",
        "        loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print(f\"Test Accuracy: {acc:.4f}\")\n",
        "\n",
        "        # Collect popup score statistics\n",
        "        score_stats = {}\n",
        "        for layer in model.layers:\n",
        "            if isinstance(layer, SubnetConv2D):\n",
        "                popup_scores = layer.popup_scores.numpy()\n",
        "                stats = {\n",
        "                    'min': np.min(popup_scores),\n",
        "                    'max': np.max(popup_scores),\n",
        "                    'mean': np.mean(popup_scores),\n",
        "                    'std': np.std(popup_scores)\n",
        "                }\n",
        "                score_stats[layer.name] = stats\n",
        "                print(f\"[{layer.name}] Popup Scores - min: {stats['min']:.4f}, max: {stats['max']:.4f}, mean: {stats['mean']:.4f}, std: {stats['std']:.4f}\")\n",
        "\n",
        "                # Optional: save popup scores for debugging\n",
        "                np.save(os.path.join(output_dir, f\"popup_scores_iter{i+1}_{layer.name}.npy\"), popup_scores)\n",
        "\n",
        "        acc_hist.append(acc)\n",
        "        size_hist.append(len(labeled))\n",
        "        popup_log.append(score_stats)\n",
        "\n",
        "        # Save the model from the final iteration\n",
        "        if i == iterations - 1:\n",
        "            final_subnet_model = model\n",
        "            model.save_weights(final_subnet_weights)\n",
        "            print(f\"Final subnet model saved to {final_subnet_weights}\")\n",
        "\n",
        "        if i < iterations - 1:\n",
        "            x_pool = x_train[unlabeled]\n",
        "            indices_new = least_confidence_sampling(model, x_pool, query_size)\n",
        "            new_samples = unlabeled[indices_new]\n",
        "            labeled = np.concatenate([labeled, new_samples])\n",
        "            unlabeled = np.setdiff1d(unlabeled, new_samples)\n",
        "\n",
        "    # Save accuracy log\n",
        "    pd.DataFrame({\n",
        "        \"samples\": size_hist,\n",
        "        \"accuracy\": acc_hist\n",
        "    }).to_csv(log_file, index=False)\n",
        "\n",
        "    # Save popup score stats\n",
        "    pd.DataFrame([\n",
        "        {\"iteration\": i+1, \"layer\": layer, **popup_log[i][layer]}\n",
        "        for i in range(len(popup_log)) for layer in popup_log[i]\n",
        "    ]).to_csv(os.path.join(output_dir, \"popup_score_stats.csv\"), index=False)\n",
        "\n",
        "    plot_progress(acc_hist, size_hist)\n",
        "    return acc_hist, size_hist, final_subnet_model\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Full Dataset Training\n",
        "# ---------------------------\n",
        "\n",
        "def train_models_on_full_dataset(global_model, final_subnet_model, x_train, y_train, x_test, y_test, epochs=20):\n",
        "    \"\"\"Train both the global model and final subnet model on the full dataset for comparison\"\"\"\n",
        "    print(\"\\n=== Training Global LeNet-5 on Full Dataset ===\")\n",
        "\n",
        "    # Create a fresh copy of the global model for full training\n",
        "    global_model_full = build_global_lenet5()\n",
        "    global_model_full.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    global_model_full.fit(x_train, y_train, epochs=epochs, batch_size=128, validation_split=0.1, verbose=1)\n",
        "    global_model_full.save_weights(final_global_weights)\n",
        "\n",
        "    global_loss, global_acc = global_model_full.evaluate(x_test, y_test, verbose=0)\n",
        "    print(f\"Global LeNet-5 Test Accuracy after full training: {global_acc:.4f}\")\n",
        "\n",
        "    print(\"\\n=== Training Final Subnet Model on Full Dataset ===\")\n",
        "    # Reset the final subnet model and train on full dataset\n",
        "    final_subnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    final_subnet_model.fit(x_train, y_train, epochs=epochs, batch_size=128, validation_split=0.1, verbose=1)\n",
        "\n",
        "    subnet_loss, subnet_acc = final_subnet_model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(f\"Final Subnet Test Accuracy after full training: {subnet_acc:.4f}\")\n",
        "\n",
        "    # Compare the models\n",
        "    print(\"\\n=== Model Comparison ===\")\n",
        "    print(f\"Global LeNet-5: {global_acc:.4f}\")\n",
        "    print(f\"Final Subnet: {subnet_acc:.4f}\")\n",
        "    print(f\"Difference: {subnet_acc - global_acc:.4f}\")\n",
        "\n",
        "    # Save comparison results\n",
        "    comparison_df = pd.DataFrame({\n",
        "        \"model\": [\"Global LeNet-5\", \"Final Subnet\"],\n",
        "        \"accuracy\": [global_acc, subnet_acc]\n",
        "    })\n",
        "    comparison_df.to_csv(os.path.join(output_dir, \"model_comparison.csv\"), index=False)\n",
        "\n",
        "    # Plot comparison\n",
        "    plot_comparison(global_acc, subnet_acc)\n",
        "\n",
        "    return global_acc, subnet_acc\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Run the Setup\n",
        "# ---------------------------\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Subset for speed\n",
        "num_train_samples = 30000\n",
        "x_train = x_train[:num_train_samples]\n",
        "y_train = y_train[:num_train_samples]\n",
        "\n",
        "# Check if pretrained weights exist\n",
        "if not os.path.exists(weights_file):\n",
        "    print(\"Training and saving global LeNet-5 model...\")\n",
        "    global_lenet5 = build_global_lenet5()\n",
        "    global_lenet5.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    global_lenet5.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.1)\n",
        "    global_lenet5.save_weights(weights_file)\n",
        "else:\n",
        "    print(\"Loading pretrained LeNet-5 weights...\")\n",
        "    global_lenet5 = build_global_lenet5()\n",
        "    global_lenet5.load_weights(weights_file)\n",
        "\n",
        "# Run Active Learning with pretrained global model\n",
        "acc_history, size_history, final_subnet_model = active_learning(\n",
        "    global_lenet5, x_train, y_train, x_test, y_test,\n",
        "    k=0.5, init_size=2000, query_size=2000, iterations=15, epochs=10\n",
        ")\n",
        "\n",
        "for i, (n, acc) in enumerate(zip(size_history, acc_history)):\n",
        "    print(f\"Iteration {i+1}: {n} samples - Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Train both models on the full dataset for final comparison\n",
        "global_acc, subnet_acc = train_models_on_full_dataset(\n",
        "    global_lenet5, final_subnet_model, x_train, y_train, x_test, y_test, epochs=20\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypzvsZ6hq8Hf",
        "outputId": "a1c2f9b1-40da-45ca-ac29-e648029b86db"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 0us/step\n",
            "Training and saving global LeNet-5 model...\n",
            "Epoch 1/10\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.2846 - loss: 1.9842 - val_accuracy: 0.3563 - val_loss: 1.7957\n",
            "Epoch 2/10\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3901 - loss: 1.7349 - val_accuracy: 0.3867 - val_loss: 1.6915\n",
            "Epoch 3/10\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4236 - loss: 1.6512 - val_accuracy: 0.4313 - val_loss: 1.6048\n",
            "Epoch 4/10\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4483 - loss: 1.5586 - val_accuracy: 0.4423 - val_loss: 1.5394\n",
            "Epoch 5/10\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4773 - loss: 1.4786 - val_accuracy: 0.4763 - val_loss: 1.4832\n",
            "Epoch 6/10\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5016 - loss: 1.4059 - val_accuracy: 0.4903 - val_loss: 1.4405\n",
            "Epoch 7/10\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5258 - loss: 1.3407 - val_accuracy: 0.4853 - val_loss: 1.4235\n",
            "Epoch 8/10\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5348 - loss: 1.3125 - val_accuracy: 0.4910 - val_loss: 1.4117\n",
            "Epoch 9/10\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5631 - loss: 1.2429 - val_accuracy: 0.4983 - val_loss: 1.3967\n",
            "Epoch 10/10\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5747 - loss: 1.2164 - val_accuracy: 0.4977 - val_loss: 1.4092\n",
            "\n",
            "=== Iteration 1 ===\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['subnet_conv2d/popup_scores', 'subnet_conv2d_1/popup_scores'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 316ms/step - accuracy: 0.1441 - loss: 2.3294 - val_accuracy: 0.2100 - val_loss: 2.1190\n",
            "Epoch 2/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2486 - loss: 2.0487 - val_accuracy: 0.2700 - val_loss: 2.0331\n",
            "Epoch 3/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3121 - loss: 1.9363 - val_accuracy: 0.2650 - val_loss: 1.9786\n",
            "Epoch 4/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3512 - loss: 1.8527 - val_accuracy: 0.3000 - val_loss: 1.9361\n",
            "Epoch 5/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3612 - loss: 1.7947 - val_accuracy: 0.3050 - val_loss: 1.9608\n",
            "Epoch 6/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3855 - loss: 1.7800 - val_accuracy: 0.3200 - val_loss: 1.9879\n",
            "Epoch 7/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3895 - loss: 1.7758 - val_accuracy: 0.2900 - val_loss: 1.9445\n",
            "Epoch 8/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3853 - loss: 1.7613 - val_accuracy: 0.2950 - val_loss: 1.9625\n",
            "Epoch 9/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3943 - loss: 1.7146 - val_accuracy: 0.3150 - val_loss: 1.9313\n",
            "Epoch 10/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4254 - loss: 1.6894 - val_accuracy: 0.3000 - val_loss: 1.9248\n",
            "Test Accuracy: 0.3568\n",
            "[subnet_conv2d] Popup Scores - min: -0.3114, max: 0.2621, mean: -0.0044, std: 0.0973\n",
            "[subnet_conv2d_1] Popup Scores - min: -0.3046, max: 0.3582, mean: 0.0015, std: 0.0984\n",
            "\n",
            "=== Iteration 2 ===\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['subnet_conv2d_2/popup_scores', 'subnet_conv2d_3/popup_scores'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 156ms/step - accuracy: 0.1428 - loss: 2.2771 - val_accuracy: 0.1550 - val_loss: 2.2041\n",
            "Epoch 2/10\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.2455 - loss: 2.0546 - val_accuracy: 0.2050 - val_loss: 2.1285\n",
            "Epoch 3/10\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2605 - loss: 2.0187 - val_accuracy: 0.1950 - val_loss: 2.0969\n",
            "Epoch 4/10\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3011 - loss: 1.9367 - val_accuracy: 0.2350 - val_loss: 2.0680\n",
            "Epoch 5/10\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3041 - loss: 1.9191 - val_accuracy: 0.2425 - val_loss: 2.0664\n",
            "Epoch 6/10\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2988 - loss: 1.9112 - val_accuracy: 0.1900 - val_loss: 2.0703\n",
            "Epoch 7/10\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3209 - loss: 1.9085 - val_accuracy: 0.2850 - val_loss: 2.0245\n",
            "Epoch 8/10\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3335 - loss: 1.8567 - val_accuracy: 0.2550 - val_loss: 2.0802\n",
            "Epoch 9/10\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3282 - loss: 1.8669 - val_accuracy: 0.2050 - val_loss: 2.0790\n",
            "Epoch 10/10\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3392 - loss: 1.8236 - val_accuracy: 0.2350 - val_loss: 2.0231\n",
            "Test Accuracy: 0.3745\n",
            "[subnet_conv2d_2] Popup Scores - min: -0.4147, max: 0.2422, mean: 0.0009, std: 0.0994\n",
            "[subnet_conv2d_3] Popup Scores - min: -0.3369, max: 0.3300, mean: -0.0004, std: 0.0986\n",
            "\n",
            "=== Iteration 3 ===\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['subnet_conv2d_4/popup_scores', 'subnet_conv2d_5/popup_scores'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 109ms/step - accuracy: 0.1748 - loss: 2.2149 - val_accuracy: 0.2033 - val_loss: 2.1273\n",
            "Epoch 2/10\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.2508 - loss: 2.0382 - val_accuracy: 0.2233 - val_loss: 2.0902\n",
            "Epoch 3/10\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2960 - loss: 1.9649 - val_accuracy: 0.2183 - val_loss: 2.0672\n",
            "Epoch 4/10\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2967 - loss: 1.9543 - val_accuracy: 0.2333 - val_loss: 2.0548\n",
            "Epoch 5/10\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3135 - loss: 1.9124 - val_accuracy: 0.2383 - val_loss: 2.0443\n",
            "Epoch 6/10\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3244 - loss: 1.8964 - val_accuracy: 0.2300 - val_loss: 2.0267\n",
            "Epoch 7/10\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3269 - loss: 1.8599 - val_accuracy: 0.2417 - val_loss: 2.0090\n",
            "Epoch 8/10\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3334 - loss: 1.8529 - val_accuracy: 0.2567 - val_loss: 1.9941\n",
            "Epoch 9/10\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3605 - loss: 1.8223 - val_accuracy: 0.2767 - val_loss: 1.9807\n",
            "Epoch 10/10\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3536 - loss: 1.8019 - val_accuracy: 0.2767 - val_loss: 1.9689\n",
            "Test Accuracy: 0.3972\n",
            "[subnet_conv2d_4] Popup Scores - min: -0.3717, max: 0.2900, mean: -0.0052, std: 0.1067\n",
            "[subnet_conv2d_5] Popup Scores - min: -0.3228, max: 0.3393, mean: -0.0012, std: 0.0985\n",
            "\n",
            "=== Iteration 4 ===\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['subnet_conv2d_6/popup_scores', 'subnet_conv2d_7/popup_scores'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 83ms/step - accuracy: 0.1602 - loss: 2.2535 - val_accuracy: 0.2325 - val_loss: 2.0663\n",
            "Epoch 2/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.2323 - loss: 2.0780 - val_accuracy: 0.2387 - val_loss: 2.0928\n",
            "Epoch 3/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2548 - loss: 2.0429 - val_accuracy: 0.2000 - val_loss: 2.0578\n",
            "Epoch 4/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2590 - loss: 2.0169 - val_accuracy: 0.2812 - val_loss: 2.0266\n",
            "Epoch 5/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2810 - loss: 1.9726 - val_accuracy: 0.2625 - val_loss: 2.0197\n",
            "Epoch 6/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2788 - loss: 1.9698 - val_accuracy: 0.2325 - val_loss: 2.0372\n",
            "Epoch 7/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2958 - loss: 1.9430 - val_accuracy: 0.2763 - val_loss: 1.9860\n",
            "Epoch 8/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2963 - loss: 1.9319 - val_accuracy: 0.2612 - val_loss: 1.9855\n",
            "Epoch 9/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3166 - loss: 1.9096 - val_accuracy: 0.2775 - val_loss: 1.9655\n",
            "Epoch 10/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3184 - loss: 1.8784 - val_accuracy: 0.2850 - val_loss: 1.9607\n",
            "Test Accuracy: 0.3875\n",
            "[subnet_conv2d_6] Popup Scores - min: -0.2648, max: 0.2800, mean: -0.0011, std: 0.0943\n",
            "[subnet_conv2d_7] Popup Scores - min: -0.3614, max: 0.3063, mean: 0.0005, std: 0.1003\n",
            "\n",
            "=== Iteration 5 ===\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['subnet_conv2d_8/popup_scores', 'subnet_conv2d_9/popup_scores'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 80ms/step - accuracy: 0.1542 - loss: 2.2503 - val_accuracy: 0.1930 - val_loss: 2.1257\n",
            "Epoch 2/10\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2178 - loss: 2.0977 - val_accuracy: 0.2310 - val_loss: 2.1019\n",
            "Epoch 3/10\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2435 - loss: 2.0499 - val_accuracy: 0.2140 - val_loss: 2.0749\n",
            "Epoch 4/10\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2433 - loss: 2.0347 - val_accuracy: 0.2400 - val_loss: 2.0388\n",
            "Epoch 5/10\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2790 - loss: 1.9842 - val_accuracy: 0.2840 - val_loss: 1.9863\n",
            "Epoch 6/10\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2853 - loss: 1.9451 - val_accuracy: 0.2430 - val_loss: 1.9855\n",
            "Epoch 7/10\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2943 - loss: 1.9271 - val_accuracy: 0.2970 - val_loss: 1.9125\n",
            "Epoch 8/10\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3205 - loss: 1.8905 - val_accuracy: 0.2870 - val_loss: 1.9215\n",
            "Epoch 9/10\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3277 - loss: 1.8587 - val_accuracy: 0.3260 - val_loss: 1.8817\n",
            "Epoch 10/10\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3238 - loss: 1.8366 - val_accuracy: 0.3170 - val_loss: 1.8807\n",
            "Test Accuracy: 0.4106\n",
            "[subnet_conv2d_8] Popup Scores - min: -0.3053, max: 0.2578, mean: -0.0025, std: 0.0984\n",
            "[subnet_conv2d_9] Popup Scores - min: -0.3405, max: 0.3560, mean: -0.0007, std: 0.1002\n",
            "\n",
            "=== Iteration 6 ===\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['subnet_conv2d_10/popup_scores', 'subnet_conv2d_11/popup_scores'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 0.1792 - loss: 2.1961 - val_accuracy: 0.2617 - val_loss: 2.0195\n",
            "Epoch 2/10\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.2538 - loss: 2.0389 - val_accuracy: 0.2675 - val_loss: 1.9775\n",
            "Epoch 3/10\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2751 - loss: 1.9814 - val_accuracy: 0.3000 - val_loss: 1.8946\n",
            "Epoch 4/10\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.2904 - loss: 1.9618 - val_accuracy: 0.3258 - val_loss: 1.8780\n",
            "Epoch 5/10\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3185 - loss: 1.9105 - val_accuracy: 0.3183 - val_loss: 1.8388\n",
            "Epoch 6/10\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3246 - loss: 1.8723 - val_accuracy: 0.3417 - val_loss: 1.8101\n",
            "Epoch 7/10\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3420 - loss: 1.8213 - val_accuracy: 0.3625 - val_loss: 1.7801\n",
            "Epoch 8/10\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3552 - loss: 1.7850 - val_accuracy: 0.3308 - val_loss: 1.7829\n",
            "Epoch 9/10\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3578 - loss: 1.7594 - val_accuracy: 0.3550 - val_loss: 1.7681\n",
            "Epoch 10/10\n",
            "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3707 - loss: 1.7360 - val_accuracy: 0.3558 - val_loss: 1.7324\n",
            "Test Accuracy: 0.4387\n",
            "[subnet_conv2d_10] Popup Scores - min: -0.2496, max: 0.3162, mean: 0.0051, std: 0.1059\n",
            "[subnet_conv2d_11] Popup Scores - min: -0.3513, max: 0.3290, mean: -0.0026, std: 0.0990\n",
            "\n",
            "=== Iteration 7 ===\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['subnet_conv2d_12/popup_scores', 'subnet_conv2d_13/popup_scores'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.1668 - loss: 2.2106 - val_accuracy: 0.2057 - val_loss: 2.0127\n",
            "Epoch 2/10\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2343 - loss: 2.0643 - val_accuracy: 0.2314 - val_loss: 1.9903\n",
            "Epoch 3/10\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2573 - loss: 2.0149 - val_accuracy: 0.2157 - val_loss: 1.9777\n",
            "Epoch 4/10\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2620 - loss: 1.9799 - val_accuracy: 0.2536 - val_loss: 1.9179\n",
            "Epoch 5/10\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.2915 - loss: 1.9439 - val_accuracy: 0.2986 - val_loss: 1.8860\n",
            "Epoch 6/10\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2978 - loss: 1.9156 - val_accuracy: 0.3079 - val_loss: 1.8713\n",
            "Epoch 7/10\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3215 - loss: 1.8704 - val_accuracy: 0.3143 - val_loss: 1.8257\n",
            "Epoch 8/10\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3227 - loss: 1.8542 - val_accuracy: 0.2950 - val_loss: 1.8444\n",
            "Epoch 9/10\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3395 - loss: 1.8050 - val_accuracy: 0.3021 - val_loss: 1.8356\n",
            "Epoch 10/10\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3482 - loss: 1.7898 - val_accuracy: 0.3293 - val_loss: 1.7933\n",
            "Test Accuracy: 0.4284\n",
            "[subnet_conv2d_12] Popup Scores - min: -0.2463, max: 0.3403, mean: -0.0090, std: 0.0938\n",
            "[subnet_conv2d_13] Popup Scores - min: -0.3302, max: 0.3253, mean: -0.0013, std: 0.0993\n",
            "\n",
            "=== Iteration 8 ===\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['subnet_conv2d_14/popup_scores', 'subnet_conv2d_15/popup_scores'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - accuracy: 0.1671 - loss: 2.2148 - val_accuracy: 0.2537 - val_loss: 1.9811\n",
            "Epoch 2/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2493 - loss: 2.0404 - val_accuracy: 0.3031 - val_loss: 1.9209\n",
            "Epoch 3/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.2669 - loss: 1.9946 - val_accuracy: 0.3050 - val_loss: 1.8912\n",
            "Epoch 4/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.2901 - loss: 1.9369 - val_accuracy: 0.3175 - val_loss: 1.8586\n",
            "Epoch 5/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3119 - loss: 1.8978 - val_accuracy: 0.3294 - val_loss: 1.8155\n",
            "Epoch 6/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3290 - loss: 1.8449 - val_accuracy: 0.3481 - val_loss: 1.7899\n",
            "Epoch 7/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3420 - loss: 1.8215 - val_accuracy: 0.3581 - val_loss: 1.7665\n",
            "Epoch 8/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3490 - loss: 1.7814 - val_accuracy: 0.3600 - val_loss: 1.7750\n",
            "Epoch 9/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3564 - loss: 1.7629 - val_accuracy: 0.3700 - val_loss: 1.7407\n",
            "Epoch 10/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3682 - loss: 1.7439 - val_accuracy: 0.3800 - val_loss: 1.7162\n",
            "Test Accuracy: 0.4451\n",
            "[subnet_conv2d_14] Popup Scores - min: -0.2881, max: 0.3298, mean: -0.0003, std: 0.0981\n",
            "[subnet_conv2d_15] Popup Scores - min: -0.3575, max: 0.3320, mean: -0.0005, std: 0.1004\n",
            "\n",
            "=== Iteration 9 ===\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['subnet_conv2d_16/popup_scores', 'subnet_conv2d_17/popup_scores'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 44ms/step - accuracy: 0.1725 - loss: 2.1852 - val_accuracy: 0.3006 - val_loss: 1.8452\n",
            "Epoch 2/10\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.2590 - loss: 2.0077 - val_accuracy: 0.3439 - val_loss: 1.7704\n",
            "Epoch 3/10\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.2961 - loss: 1.9417 - val_accuracy: 0.3439 - val_loss: 1.7264\n",
            "Epoch 4/10\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3078 - loss: 1.8961 - val_accuracy: 0.3578 - val_loss: 1.6753\n",
            "Epoch 5/10\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3411 - loss: 1.8212 - val_accuracy: 0.3544 - val_loss: 1.6558\n",
            "Epoch 6/10\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3572 - loss: 1.7819 - val_accuracy: 0.3650 - val_loss: 1.6336\n",
            "Epoch 7/10\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3587 - loss: 1.7671 - val_accuracy: 0.3861 - val_loss: 1.6098\n",
            "Epoch 8/10\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3728 - loss: 1.7154 - val_accuracy: 0.3928 - val_loss: 1.5900\n",
            "Epoch 9/10\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3851 - loss: 1.6825 - val_accuracy: 0.4144 - val_loss: 1.5739\n",
            "Epoch 10/10\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4145 - loss: 1.6359 - val_accuracy: 0.4206 - val_loss: 1.5498\n",
            "Test Accuracy: 0.4509\n",
            "[subnet_conv2d_16] Popup Scores - min: -0.3316, max: 0.2847, mean: -0.0047, std: 0.1033\n",
            "[subnet_conv2d_17] Popup Scores - min: -0.2951, max: 0.3350, mean: 0.0009, std: 0.0974\n",
            "\n",
            "=== Iteration 10 ===\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['subnet_conv2d_18/popup_scores', 'subnet_conv2d_19/popup_scores'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 35ms/step - accuracy: 0.2028 - loss: 2.1445 - val_accuracy: 0.3205 - val_loss: 1.8290\n",
            "Epoch 2/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.2813 - loss: 1.9650 - val_accuracy: 0.3225 - val_loss: 1.8066\n",
            "Epoch 3/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.2970 - loss: 1.9168 - val_accuracy: 0.3530 - val_loss: 1.7807\n",
            "Epoch 4/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3112 - loss: 1.8754 - val_accuracy: 0.3575 - val_loss: 1.7254\n",
            "Epoch 5/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3309 - loss: 1.8296 - val_accuracy: 0.3825 - val_loss: 1.6792\n",
            "Epoch 6/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3552 - loss: 1.7860 - val_accuracy: 0.4000 - val_loss: 1.6464\n",
            "Epoch 7/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3613 - loss: 1.7497 - val_accuracy: 0.3940 - val_loss: 1.6270\n",
            "Epoch 8/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3654 - loss: 1.7269 - val_accuracy: 0.4170 - val_loss: 1.6129\n",
            "Epoch 9/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3819 - loss: 1.6908 - val_accuracy: 0.4140 - val_loss: 1.5842\n",
            "Epoch 10/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3865 - loss: 1.6750 - val_accuracy: 0.4210 - val_loss: 1.5848\n",
            "Test Accuracy: 0.4613\n",
            "[subnet_conv2d_18] Popup Scores - min: -0.2826, max: 0.2739, mean: 0.0080, std: 0.1003\n",
            "[subnet_conv2d_19] Popup Scores - min: -0.3536, max: 0.3316, mean: 0.0027, std: 0.1007\n",
            "\n",
            "=== Iteration 11 ===\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['subnet_conv2d_20/popup_scores', 'subnet_conv2d_21/popup_scores'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 31ms/step - accuracy: 0.1997 - loss: 2.1388 - val_accuracy: 0.3809 - val_loss: 1.7183\n",
            "Epoch 2/10\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.2821 - loss: 1.9616 - val_accuracy: 0.3977 - val_loss: 1.6773\n",
            "Epoch 3/10\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.2933 - loss: 1.9194 - val_accuracy: 0.4100 - val_loss: 1.6413\n",
            "Epoch 4/10\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3195 - loss: 1.8625 - val_accuracy: 0.4118 - val_loss: 1.5987\n",
            "Epoch 5/10\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3400 - loss: 1.8096 - val_accuracy: 0.4100 - val_loss: 1.5909\n",
            "Epoch 6/10\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3495 - loss: 1.7747 - val_accuracy: 0.4427 - val_loss: 1.5210\n",
            "Epoch 7/10\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3700 - loss: 1.7286 - val_accuracy: 0.4659 - val_loss: 1.4923\n",
            "Epoch 8/10\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3872 - loss: 1.6979 - val_accuracy: 0.4532 - val_loss: 1.4789\n",
            "Epoch 9/10\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3953 - loss: 1.6690 - val_accuracy: 0.4873 - val_loss: 1.4403\n",
            "Epoch 10/10\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4099 - loss: 1.6262 - val_accuracy: 0.4768 - val_loss: 1.4320\n",
            "Test Accuracy: 0.4696\n",
            "[subnet_conv2d_20] Popup Scores - min: -0.3049, max: 0.2558, mean: 0.0090, std: 0.1000\n",
            "[subnet_conv2d_21] Popup Scores - min: -0.3666, max: 0.2971, mean: -0.0020, std: 0.0982\n",
            "\n",
            "=== Iteration 12 ===\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['subnet_conv2d_22/popup_scores', 'subnet_conv2d_23/popup_scores'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 29ms/step - accuracy: 0.2213 - loss: 2.0805 - val_accuracy: 0.4304 - val_loss: 1.6074\n",
            "Epoch 2/10\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.3057 - loss: 1.9000 - val_accuracy: 0.4779 - val_loss: 1.4967\n",
            "Epoch 3/10\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3277 - loss: 1.8279 - val_accuracy: 0.4921 - val_loss: 1.4397\n",
            "Epoch 4/10\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3529 - loss: 1.7594 - val_accuracy: 0.4971 - val_loss: 1.3853\n",
            "Epoch 5/10\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3688 - loss: 1.7174 - val_accuracy: 0.5108 - val_loss: 1.3517\n",
            "Epoch 6/10\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3875 - loss: 1.6754 - val_accuracy: 0.5029 - val_loss: 1.3323\n",
            "Epoch 7/10\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4065 - loss: 1.6337 - val_accuracy: 0.5158 - val_loss: 1.3149\n",
            "Epoch 8/10\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4172 - loss: 1.5877 - val_accuracy: 0.5104 - val_loss: 1.3014\n",
            "Epoch 9/10\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4323 - loss: 1.5587 - val_accuracy: 0.5392 - val_loss: 1.2568\n",
            "Epoch 10/10\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4448 - loss: 1.5304 - val_accuracy: 0.5471 - val_loss: 1.2480\n",
            "Test Accuracy: 0.4909\n",
            "[subnet_conv2d_22] Popup Scores - min: -0.3889, max: 0.3139, mean: -0.0025, std: 0.0988\n",
            "[subnet_conv2d_23] Popup Scores - min: -0.3914, max: 0.3454, mean: -0.0027, std: 0.0987\n",
            "\n",
            "=== Iteration 13 ===\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['subnet_conv2d_24/popup_scores', 'subnet_conv2d_25/popup_scores'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.2323 - loss: 2.0590 - val_accuracy: 0.5027 - val_loss: 1.4224\n",
            "Epoch 2/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3179 - loss: 1.8709 - val_accuracy: 0.5235 - val_loss: 1.3523\n",
            "Epoch 3/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3464 - loss: 1.8016 - val_accuracy: 0.5538 - val_loss: 1.2532\n",
            "Epoch 4/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3663 - loss: 1.7396 - val_accuracy: 0.5735 - val_loss: 1.2299\n",
            "Epoch 5/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3812 - loss: 1.6940 - val_accuracy: 0.5808 - val_loss: 1.1747\n",
            "Epoch 6/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4073 - loss: 1.6335 - val_accuracy: 0.5900 - val_loss: 1.1530\n",
            "Epoch 7/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4194 - loss: 1.6018 - val_accuracy: 0.5854 - val_loss: 1.1401\n",
            "Epoch 8/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4270 - loss: 1.5795 - val_accuracy: 0.5719 - val_loss: 1.1487\n",
            "Epoch 9/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4412 - loss: 1.5394 - val_accuracy: 0.5896 - val_loss: 1.1168\n",
            "Epoch 10/10\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4550 - loss: 1.5055 - val_accuracy: 0.5750 - val_loss: 1.1227\n",
            "Test Accuracy: 0.4916\n",
            "[subnet_conv2d_24] Popup Scores - min: -0.3077, max: 0.2919, mean: 0.0041, std: 0.0990\n",
            "[subnet_conv2d_25] Popup Scores - min: -0.2947, max: 0.3317, mean: 0.0014, std: 0.0997\n",
            "\n",
            "=== Iteration 14 ===\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['subnet_conv2d_26/popup_scores', 'subnet_conv2d_27/popup_scores'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - accuracy: 0.2467 - loss: 2.0379 - val_accuracy: 0.5989 - val_loss: 1.2656\n",
            "Epoch 2/10\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3273 - loss: 1.8527 - val_accuracy: 0.6754 - val_loss: 1.1634\n",
            "Epoch 3/10\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3583 - loss: 1.7644 - val_accuracy: 0.7118 - val_loss: 1.0774\n",
            "Epoch 4/10\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3869 - loss: 1.7084 - val_accuracy: 0.7143 - val_loss: 1.0210\n",
            "Epoch 5/10\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3942 - loss: 1.6584 - val_accuracy: 0.7236 - val_loss: 0.9711\n",
            "Epoch 6/10\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4160 - loss: 1.6158 - val_accuracy: 0.7100 - val_loss: 0.9834\n",
            "Epoch 7/10\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4344 - loss: 1.5623 - val_accuracy: 0.7368 - val_loss: 0.9064\n",
            "Epoch 8/10\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4513 - loss: 1.5183 - val_accuracy: 0.7343 - val_loss: 0.9049\n",
            "Epoch 9/10\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4643 - loss: 1.4867 - val_accuracy: 0.7375 - val_loss: 0.8756\n",
            "Epoch 10/10\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4682 - loss: 1.4620 - val_accuracy: 0.7443 - val_loss: 0.8578\n",
            "Test Accuracy: 0.4998\n",
            "[subnet_conv2d_26] Popup Scores - min: -0.3914, max: 0.2885, mean: -0.0097, std: 0.0983\n",
            "[subnet_conv2d_27] Popup Scores - min: -0.3383, max: 0.3736, mean: 0.0010, std: 0.0995\n",
            "\n",
            "=== Iteration 15 ===\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['subnet_conv2d_28/popup_scores', 'subnet_conv2d_29/popup_scores'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.2633 - loss: 2.0103 - val_accuracy: 0.7787 - val_loss: 1.0322\n",
            "Epoch 2/10\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3476 - loss: 1.8142 - val_accuracy: 0.8210 - val_loss: 0.8891\n",
            "Epoch 3/10\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3673 - loss: 1.7452 - val_accuracy: 0.8297 - val_loss: 0.8615\n",
            "Epoch 4/10\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3961 - loss: 1.6757 - val_accuracy: 0.8243 - val_loss: 0.7857\n",
            "Epoch 5/10\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4161 - loss: 1.6082 - val_accuracy: 0.8507 - val_loss: 0.7123\n",
            "Epoch 6/10\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4332 - loss: 1.5728 - val_accuracy: 0.8477 - val_loss: 0.7088\n",
            "Epoch 7/10\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4512 - loss: 1.5228 - val_accuracy: 0.8533 - val_loss: 0.6620\n",
            "Epoch 8/10\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4580 - loss: 1.4979 - val_accuracy: 0.8570 - val_loss: 0.6367\n",
            "Epoch 9/10\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4736 - loss: 1.4554 - val_accuracy: 0.8517 - val_loss: 0.6250\n",
            "Epoch 10/10\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4731 - loss: 1.4381 - val_accuracy: 0.8523 - val_loss: 0.6315\n",
            "Test Accuracy: 0.4902\n",
            "[subnet_conv2d_28] Popup Scores - min: -0.3094, max: 0.2940, mean: -0.0024, std: 0.1025\n",
            "[subnet_conv2d_29] Popup Scores - min: -0.3414, max: 0.3441, mean: -0.0017, std: 0.0984\n",
            "Final subnet model saved to /content/drive/MyDrive/Edge-Pop_0.5_2/final_subnet.weights.h5\n",
            "Iteration 1: 2000 samples - Accuracy: 0.3568\n",
            "Iteration 2: 4000 samples - Accuracy: 0.3745\n",
            "Iteration 3: 6000 samples - Accuracy: 0.3972\n",
            "Iteration 4: 8000 samples - Accuracy: 0.3875\n",
            "Iteration 5: 10000 samples - Accuracy: 0.4106\n",
            "Iteration 6: 12000 samples - Accuracy: 0.4387\n",
            "Iteration 7: 14000 samples - Accuracy: 0.4284\n",
            "Iteration 8: 16000 samples - Accuracy: 0.4451\n",
            "Iteration 9: 18000 samples - Accuracy: 0.4509\n",
            "Iteration 10: 20000 samples - Accuracy: 0.4613\n",
            "Iteration 11: 22000 samples - Accuracy: 0.4696\n",
            "Iteration 12: 24000 samples - Accuracy: 0.4909\n",
            "Iteration 13: 26000 samples - Accuracy: 0.4916\n",
            "Iteration 14: 28000 samples - Accuracy: 0.4998\n",
            "Iteration 15: 30000 samples - Accuracy: 0.4902\n",
            "\n",
            "=== Training Global LeNet-5 on Full Dataset ===\n",
            "Epoch 1/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.2819 - loss: 2.0002 - val_accuracy: 0.3663 - val_loss: 1.7754\n",
            "Epoch 2/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3908 - loss: 1.7483 - val_accuracy: 0.3963 - val_loss: 1.7112\n",
            "Epoch 3/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4209 - loss: 1.6701 - val_accuracy: 0.4143 - val_loss: 1.6286\n",
            "Epoch 4/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4466 - loss: 1.5801 - val_accuracy: 0.4370 - val_loss: 1.5652\n",
            "Epoch 5/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4756 - loss: 1.4907 - val_accuracy: 0.4717 - val_loss: 1.4906\n",
            "Epoch 6/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4953 - loss: 1.4295 - val_accuracy: 0.4730 - val_loss: 1.4629\n",
            "Epoch 7/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5185 - loss: 1.3651 - val_accuracy: 0.4680 - val_loss: 1.4629\n",
            "Epoch 8/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5361 - loss: 1.2982 - val_accuracy: 0.4833 - val_loss: 1.4074\n",
            "Epoch 9/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5557 - loss: 1.2557 - val_accuracy: 0.5073 - val_loss: 1.3838\n",
            "Epoch 10/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5763 - loss: 1.2027 - val_accuracy: 0.5087 - val_loss: 1.3818\n",
            "Epoch 11/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5887 - loss: 1.1572 - val_accuracy: 0.5140 - val_loss: 1.3776\n",
            "Epoch 12/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6035 - loss: 1.1274 - val_accuracy: 0.5043 - val_loss: 1.3610\n",
            "Epoch 13/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6182 - loss: 1.0829 - val_accuracy: 0.5100 - val_loss: 1.3855\n",
            "Epoch 14/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6357 - loss: 1.0495 - val_accuracy: 0.5047 - val_loss: 1.3799\n",
            "Epoch 15/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6466 - loss: 1.0145 - val_accuracy: 0.5187 - val_loss: 1.3616\n",
            "Epoch 16/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6555 - loss: 0.9815 - val_accuracy: 0.5183 - val_loss: 1.3971\n",
            "Epoch 17/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6679 - loss: 0.9495 - val_accuracy: 0.5053 - val_loss: 1.4146\n",
            "Epoch 18/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6818 - loss: 0.9154 - val_accuracy: 0.5047 - val_loss: 1.4416\n",
            "Epoch 19/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6877 - loss: 0.8942 - val_accuracy: 0.5073 - val_loss: 1.4367\n",
            "Epoch 20/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7115 - loss: 0.8510 - val_accuracy: 0.5033 - val_loss: 1.4567\n",
            "Global LeNet-5 Test Accuracy after full training: 0.5160\n",
            "\n",
            "=== Training Final Subnet Model on Full Dataset ===\n",
            "Epoch 1/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.5280 - loss: 1.3337 - val_accuracy: 0.5147 - val_loss: 1.3508\n",
            "Epoch 2/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5376 - loss: 1.3055 - val_accuracy: 0.5287 - val_loss: 1.3331\n",
            "Epoch 3/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5463 - loss: 1.2789 - val_accuracy: 0.5120 - val_loss: 1.3648\n",
            "Epoch 4/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5457 - loss: 1.2786 - val_accuracy: 0.5257 - val_loss: 1.3275\n",
            "Epoch 5/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5587 - loss: 1.2349 - val_accuracy: 0.5253 - val_loss: 1.3310\n",
            "Epoch 6/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5647 - loss: 1.2282 - val_accuracy: 0.5397 - val_loss: 1.3081\n",
            "Epoch 7/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5745 - loss: 1.2020 - val_accuracy: 0.5200 - val_loss: 1.3219\n",
            "Epoch 8/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5782 - loss: 1.1861 - val_accuracy: 0.5300 - val_loss: 1.3173\n",
            "Epoch 9/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5846 - loss: 1.1772 - val_accuracy: 0.5307 - val_loss: 1.3117\n",
            "Epoch 10/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5859 - loss: 1.1633 - val_accuracy: 0.5270 - val_loss: 1.3182\n",
            "Epoch 11/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5929 - loss: 1.1533 - val_accuracy: 0.5270 - val_loss: 1.3210\n",
            "Epoch 12/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5951 - loss: 1.1312 - val_accuracy: 0.5210 - val_loss: 1.3314\n",
            "Epoch 13/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5957 - loss: 1.1362 - val_accuracy: 0.5250 - val_loss: 1.3343\n",
            "Epoch 14/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6109 - loss: 1.1032 - val_accuracy: 0.5377 - val_loss: 1.3113\n",
            "Epoch 15/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6100 - loss: 1.0987 - val_accuracy: 0.5250 - val_loss: 1.3350\n",
            "Epoch 16/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6128 - loss: 1.0936 - val_accuracy: 0.5233 - val_loss: 1.3551\n",
            "Epoch 17/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6210 - loss: 1.0748 - val_accuracy: 0.5197 - val_loss: 1.3420\n",
            "Epoch 18/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6182 - loss: 1.0675 - val_accuracy: 0.5320 - val_loss: 1.3523\n",
            "Epoch 19/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6270 - loss: 1.0563 - val_accuracy: 0.5353 - val_loss: 1.3220\n",
            "Epoch 20/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6316 - loss: 1.0392 - val_accuracy: 0.5380 - val_loss: 1.3257\n",
            "Final Subnet Test Accuracy after full training: 0.5221\n",
            "\n",
            "=== Model Comparison ===\n",
            "Global LeNet-5: 0.5160\n",
            "Final Subnet: 0.5221\n",
            "Difference: 0.0061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Global LeNet-5 Trainable Parameters:\", global_lenet5.count_params())\n",
        "print(\"Final Subnet Trainable Parameters:\", final_subnet_model.count_params())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyyovfWH1IdZ",
        "outputId": "47f9a521-c581-42b7-ce6e-ca4d9d5b23ff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global LeNet-5 Trainable Parameters: 83126\n",
            "Final Subnet Trainable Parameters: 83104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = np.expand_dims(x_test[0], axis=0)  # One sample from the test set\n",
        "\n",
        "global_pred = global_lenet5.predict(sample, verbose=0)\n",
        "subnet_pred = final_subnet_model.predict(sample, verbose=0)\n",
        "\n",
        "print(\"Global LeNet-5 Prediction:\", np.argmax(global_pred))\n",
        "print(\"Final Subnet Prediction:\", np.argmax(subnet_pred))\n",
        "\n",
        "print(\"Prediction Difference (L2 Norm):\", np.linalg.norm(global_pred - subnet_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p7tMy_92k10",
        "outputId": "ceeb0033-8a8d-4aed-ced8-fdef6519c6c9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global LeNet-5 Prediction: 3\n",
            "Final Subnet Prediction: 8\n",
            "Prediction Difference (L2 Norm): 0.6391015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_conv_layer_sparsity(model, model_name=\"Model\"):\n",
        "    total_params = 0\n",
        "    zero_params = 0\n",
        "\n",
        "    print(f\"\\n=== {model_name} Convolutional Layer Sparsity ===\")\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, SubnetConv2D):  # For subnet model\n",
        "            weights = layer.base_weights\n",
        "            mask = layer.get_subnet(tf.abs(layer.popup_scores)).numpy()\n",
        "            num_total = weights.size\n",
        "            num_masked = np.sum(mask == 0)\n",
        "\n",
        "            print(f\"[{layer.name}] Total: {num_total}, Masked: {num_masked}, Kept: {num_total - num_masked}\")\n",
        "            total_params += num_total\n",
        "            zero_params += num_masked\n",
        "\n",
        "        elif isinstance(layer, tf.keras.layers.Conv2D):  # For global model\n",
        "            weights = layer.get_weights()\n",
        "            if weights:\n",
        "                w = weights[0]\n",
        "                num_total = w.size\n",
        "                num_zeros = np.sum(w == 0)\n",
        "\n",
        "                print(f\"[{layer.name}] Total: {num_total}, Zeros: {num_zeros}, Non-Zero: {num_total - num_zeros}\")\n",
        "                total_params += num_total\n",
        "                zero_params += num_zeros\n",
        "\n",
        "    print(f\"\\n=== {model_name} Summary ===\")\n",
        "    print(f\"Total Conv Params : {total_params}\")\n",
        "    print(f\"Zero/Masked Params: {zero_params}\")\n",
        "    print(f\"Non-zero/Kept     : {total_params - zero_params}\")\n",
        "    print(f\"Sparsity          : {zero_params / total_params:.2%}\")\n"
      ],
      "metadata": {
        "id": "JExijQhm5OxD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_conv_layer_sparsity(final_subnet_model, model_name=\"Final Subnet Model\")\n",
        "analyze_conv_layer_sparsity(global_lenet5, model_name=\"Global LeNet-5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJf30foT5uTi",
        "outputId": "95269527-27b0-4228-dac0-bec4ac015af1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Final Subnet Model Convolutional Layer Sparsity ===\n",
            "[subnet_conv2d_28] Total: 450, Masked: 225, Kept: 225\n",
            "[subnet_conv2d_29] Total: 2400, Masked: 1200, Kept: 1200\n",
            "\n",
            "=== Final Subnet Model Summary ===\n",
            "Total Conv Params : 2850\n",
            "Zero/Masked Params: 1425\n",
            "Non-zero/Kept     : 1425\n",
            "Sparsity          : 50.00%\n",
            "\n",
            "=== Global LeNet-5 Convolutional Layer Sparsity ===\n",
            "[conv2d] Total: 450, Zeros: 0, Non-Zero: 450\n",
            "[conv2d_1] Total: 2400, Zeros: 0, Non-Zero: 2400\n",
            "\n",
            "=== Global LeNet-5 Summary ===\n",
            "Total Conv Params : 2850\n",
            "Zero/Masked Params: 0\n",
            "Non-zero/Kept     : 2850\n",
            "Sparsity          : 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in final_subnet_model.layers:\n",
        "    print(f\"{layer.name}: {layer}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-m-r00hWNdLA",
        "outputId": "4e0cb510-066f-462e-a4d7-047d0ca1137d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_layer_15: <InputLayer name=input_layer_15, built=True>\n",
            "subnet_conv2d_28: <SubnetConv2D name=subnet_conv2d_28, built=True>\n",
            "activation_28: <Activation name=activation_28, built=True>\n",
            "average_pooling2d_30: <AveragePooling2D name=average_pooling2d_30, built=True>\n",
            "subnet_conv2d_29: <SubnetConv2D name=subnet_conv2d_29, built=True>\n",
            "activation_29: <Activation name=activation_29, built=True>\n",
            "average_pooling2d_31: <AveragePooling2D name=average_pooling2d_31, built=True>\n",
            "flatten_15: <Flatten name=flatten_15, built=True>\n",
            "dense_45: <Dense name=dense_45, built=True>\n",
            "dense_46: <Dense name=dense_46, built=True>\n",
            "dense_47: <Dense name=dense_47, built=True>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_subnet_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "Gt_o5t4FRLE3",
        "outputId": "1f791a8f-c205-4da2-de7c-ddadd13ea97d"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_15\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_15\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_15 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ subnet_conv2d_28 (\u001b[38;5;33mSubnetConv2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m6\u001b[0m)      │           \u001b[38;5;34m450\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_28 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m6\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ average_pooling2d_30            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m6\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ subnet_conv2d_29 (\u001b[38;5;33mSubnetConv2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │         \u001b[38;5;34m2,400\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_29 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ average_pooling2d_31            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_15 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_45 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m)            │        \u001b[38;5;34m69,240\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_46 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m84\u001b[0m)             │        \u001b[38;5;34m10,164\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_47 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m850\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ subnet_conv2d_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SubnetConv2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">450</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ average_pooling2d_30            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ subnet_conv2d_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SubnetConv2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,400</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ average_pooling2d_31            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">69,240</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,164</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">850</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m249,314\u001b[0m (973.89 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">249,314</span> (973.89 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m83,104\u001b[0m (324.62 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">83,104</span> (324.62 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m166,210\u001b[0m (649.26 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">166,210</span> (649.26 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JKpK4f3eZlEt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}